{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "self_attention_진행중",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/ts-kim/models/blob/master/structured_self_attention.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "EabaMrsnXLJ2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##참고문헌\n",
        "**Zhouhan Lin, et al.**, *A Structured Self-attentive Sentence Embedding*\n",
        "\n",
        "https://arxiv.org/pdf/1703.03130.pdf</br>\n",
        "https://arxiv.org/abs/1703.03130\n",
        "\n",
        "\n",
        "https://simonjisu.github.io/datascience/2018/04/03/nsmcbidreclstmselfattn.html"
      ]
    },
    {
      "metadata": {
        "id": "pVuZjAU30BDZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cb_7kwRfgO6n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1315
        },
        "outputId": "73f42184-f97a-4d19-ec40-afbe526a15d1"
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "# pytorch 0.4.0 설치\n",
        "\n",
        "!pip install pytorch-nlp\n",
        "# torchnlp 설치(데이터셋 다운로드용)\n",
        "\n",
        "!pip install nltk\n",
        "!pip install gensim\n",
        "# Word2Vec 설치(워드 임베딩용)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x5bb40000 @  0x7f7296ded1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\r\n",
            "0.4.0\n",
            "True\n",
            "Collecting pytorch-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/6b/e7b42dca1f1fe69ffdcf5720eff18166a2713e3d4f5f219337b5a07500e6/pytorch_nlp-0.3.5-py3-none-any.whl (96kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.14.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (2.18.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (0.22.0)\n",
            "Collecting tqdm (from pytorch-nlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/e0/52b2faaef4fd87f86eb8a8f1afa2cd6eb11146822033e29c04ac48ada32c/tqdm-4.25.0-py2.py3-none-any.whl (43kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 17.1MB/s \n",
            "\u001b[?25hCollecting ujson (from pytorch-nlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n",
            "\u001b[K    100% |████████████████████████████████| 194kB 20.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2018.8.13)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2018.5)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas->pytorch-nlp) (1.11.0)\n",
            "Building wheels for collected packages: ujson\n",
            "  Running setup.py bdist_wheel for ujson ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n",
            "Successfully built ujson\n",
            "Installing collected packages: tqdm, ujson, pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.3.5 tqdm-4.25.0 ujson-1.35\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "Collecting gensim\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/f3/37504f07651330ddfdefa631ca5246974a60d0908216539efda842fd080f/gensim-3.5.0-cp36-cp36m-manylinux1_x86_64.whl (23.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 23.5MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Collecting smart-open>=1.2.1 (from gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/cf/3d/5f3a9a296d0ba8e00e263a8dee76762076b9eb5ddc254ccaa834651c8d65/smart_open-1.6.0.tar.gz\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Collecting boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 9.5MB/s \n",
            "\u001b[?25hCollecting bz2file (from smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/af/1f30df188ebc3d01ef3149fa816228794b440c210c221c676e024f4fb4dd/boto3-1.7.81-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K    100% |████████████████████████████████| 133kB 22.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.8.13)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 18.0MB/s \n",
            "\u001b[?25hCollecting botocore<1.11.0,>=1.10.81 (from boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/a2/a42ec424fc662aea7a582e17739ceca74102984b172676230d1bd0b36b98/botocore-1.10.81-py2.py3-none-any.whl (4.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.5MB 5.2MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
            "Collecting docutils>=0.10 (from botocore<1.11.0,>=1.10.81->boto3->smart-open>=1.2.1->gensim)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n",
            "\u001b[K    100% |████████████████████████████████| 552kB 18.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.81->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Building wheels for collected packages: smart-open, bz2file\n",
            "  Running setup.py bdist_wheel for smart-open ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/73/f1/9b/ccf93d4ba073b6f79b1ed9df68ab5ce048d8136d0efcf90b30\n",
            "  Running setup.py bdist_wheel for bz2file ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
            "Successfully built smart-open bz2file\n",
            "Installing collected packages: boto, bz2file, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\n",
            "Successfully installed boto-2.49.0 boto3-1.7.81 botocore-1.10.81 bz2file-0.98 docutils-0.14 gensim-3.5.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nDx98e_tgSz7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "50JCgAvbXeVV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##사용한 데이터\n",
        "\n",
        "The Stanford Natural Language Inference (SNLI) Corpus\n",
        "https://nlp.stanford.edu/projects/snli/"
      ]
    },
    {
      "metadata": {
        "id": "ujFLx927eam9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c8dcc1fc-ccdb-4524-940b-e71c6f07913c"
      },
      "cell_type": "code",
      "source": [
        "from torchnlp.datasets import snli_dataset\n",
        "train = snli_dataset(train=True)\n",
        "test = snli_dataset(test=True)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "snli_1.0.zip: 94.6MB [00:09, 10.3MB/s]                            \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "x13rzkZjjMsG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "192b961c-ea32-4dc7-80a3-336d3ff8b487"
      },
      "cell_type": "code",
      "source": [
        "'''print(test[0]['premise'])\n",
        "print(test[0]['label'])\n",
        "print(test[0]['hypothesis'])\n",
        "\n",
        "print()\n",
        "print(test[1]['premise'])\n",
        "print(test[1]['label'])\n",
        "print(test[1]['hypothesis'])\n",
        "\n",
        "print()\n",
        "print(test[2]['premise'])\n",
        "print(test[2]['label'])\n",
        "print(test[2]['hypothesis'])'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"print(test[0]['premise'])\\nprint(test[0]['label'])\\nprint(test[0]['hypothesis'])\\n\\nprint()\\nprint(test[1]['premise'])\\nprint(test[1]['label'])\\nprint(test[1]['hypothesis'])\\n\\nprint()\\nprint(test[2]['premise'])\\nprint(test[2]['label'])\\nprint(test[2]['hypothesis'])\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "5z-1NRflQYwu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#데이터 불필요한 부분 제거\n",
        "train_ls = list(train)\n",
        "test_ls = list(test)\n",
        "\n",
        "pd_train = pd.DataFrame(train_ls)\n",
        "pd_test = pd.DataFrame(test_ls)\n",
        "\n",
        "del pd_train['hypothesis_transitions']\n",
        "del pd_train['premise_transitions']\n",
        "del pd_train['premise']\n",
        "\n",
        "del pd_test['hypothesis_transitions']\n",
        "del pd_test['premise_transitions']\n",
        "del pd_test['premise']\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wm21LxBkj6IC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### text to vecs"
      ]
    },
    {
      "metadata": {
        "id": "7qNqdT24rkk5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2eb07132-c3b0-4ccc-c801-feec0a18b2ad"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from nltk import word_tokenize\n",
        "\n",
        "nltk.download('punkt') # this is for word_tokenizing\n",
        "\n",
        "train_sentences =[]\n",
        "test_sentences = []\n",
        "\n",
        "for i in range(len(train)):\n",
        "  train_sentences.append(train[i]['hypothesis'])\n",
        "  train_sentences[i]=word_tokenize(train_sentences[i])\n",
        "  \n",
        "for i in range(len(test)):\n",
        "  test_sentences.append(test[i]['hypothesis'])\n",
        "  test_sentences[i]=word_tokenize(test_sentences[i])\n",
        "\n",
        "sentences = train_sentences + test_sentences"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MvT3drWax6du",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "fe04bfb6-b136-4b24-844b-bd627cf96b9e"
      },
      "cell_type": "code",
      "source": [
        "# word2vec 설정\n",
        "d_dim = 100\n",
        "\n",
        "\n",
        "#word2vec 학습\n",
        "%time embedding_model = Word2Vec(sentences, size=d_dim, iter= 20, min_count = 0)\n",
        "embedding_model.save(\"word2vec.model\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 7s, sys: 1.08 s, total: 3min 9s\n",
            "Wall time: 1min 40s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gj1jNq9GrtJ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "b1dc0eca-085f-40ea-9118-844a1414bc06"
      },
      "cell_type": "code",
      "source": [
        "embedding_model = Word2Vec.load(\"word2vec.model\")\n",
        "print(embedding_model.most_similar(positive=[\"house\"], topn=5))\n",
        "#0.6 < mansion, apartment"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('mansion', 0.7233067154884338), ('apartment', 0.6885494589805603), ('cabin', 0.6371487975120544), ('workplace', 0.6230380535125732), ('office', 0.6145042181015015)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nV02OK38ABCF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "135bd38e-c96d-4190-a1e8-35554af6fab5"
      },
      "cell_type": "code",
      "source": [
        "'''print(embedding_model.wv[sentences[30]].shape) \n",
        "print('n =11(# of words in sentence), d=100(dim of word2vec) ( S = n by d dim)')'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"print(embedding_model.wv[sentences[30]].shape) \\nprint('n =11(# of words in sentence), d=100(dim of word2vec) ( S = n by d dim)')\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "U75YVUKqIRXQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "97a4853f-a1d2-4a6c-84ee-0a0582578e29"
      },
      "cell_type": "code",
      "source": [
        "'''#최대길이는? --- just for checking\n",
        "max_len=0\n",
        "for i in range(len(sentences)):\n",
        "    if len(sentences[i])>max_len:\n",
        "        max_len=len(sentences[i])\n",
        "print(max_len)'''"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#최대길이는? --- just for checking\\nmax_len=0\\nfor i in range(len(sentences)):\\n    if len(sentences[i])>max_len:\\n        max_len=len(sentences[i])\\nprint(max_len)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "3QcCCzN8qpy6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def padding(x,length): # 넘파이 어레이를 적당한 길이로 자르거나 늘려줌\n",
        "    _ = length-len(x)\n",
        "    if(_>0):\n",
        "        a = np.zeros((_,100))\n",
        "        x = np.concatenate((x,a))\n",
        "    if(_<0):\n",
        "        x = x[:length,:]\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U-1SGiOL95sA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "9df50b12-4a17-45a0-a25e-2e54f2d22269"
      },
      "cell_type": "code",
      "source": [
        "length = 9\n",
        "X_train = []\n",
        "X_test = []\n",
        "for i in range(len(train_sentences)):\n",
        "  X_train.append(padding(embedding_model.wv[train_sentences[i]],length))\n",
        "  if i%100000 ==99999 :\n",
        "    print(i+1,X_train[i].shape)\n",
        "\n",
        "for i in range(len(test_sentences)):\n",
        "  X_test.append(padding(embedding_model.wv[test_sentences[i]],length))\n",
        "print(len(test_sentences))\n",
        "print(X_test[0].shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000 (9, 100)\n",
            "200000 (9, 100)\n",
            "300000 (9, 100)\n",
            "400000 (9, 100)\n",
            "500000 (9, 100)\n",
            "10000\n",
            "(9, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KGeyhYovlyxM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a232bf3c-085e-42b5-d6c8-58ef8947fefa"
      },
      "cell_type": "code",
      "source": [
        "'''X_test[2].shape'''"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'X_test[2].shape'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "zU3B7sG5Mmc9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### y_data 준비\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "save_label = []\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "#y_train to index\n",
        "le.fit(pd_train['label'])\n",
        "save_label.append(le.classes_)\n",
        "pd_train['label'] = le.transform(pd_train['label'])\n",
        "\n",
        "#y_test to index\n",
        "le.fit(pd_test['label'])\n",
        "save_label.append(le.classes_)\n",
        "pd_test['label'] = le.transform(pd_test['label'])\n",
        "\n",
        "\n",
        "y_train = pd_train['label']\n",
        "y_test = pd_test['label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "njeA9cDKH7Si",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train).type(torch.FloatTensor).to(device)\n",
        "y_train = torch.tensor(y_train.values).type(torch.LongTensor).to(device)\n",
        "\n",
        "X_test = torch.tensor(X_test).type(torch.FloatTensor).to(device)\n",
        "y_test = torch.tensor(y_test.values).type(torch.LongTensor).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JHKgexyS83xl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train) # X_train : float tensor, y_train : long tensor\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FEhYPqCTuaau",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# model  "
      ]
    },
    {
      "metadata": {
        "id": "krggIKn8CJSx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#hyper parameters\n",
        "input_size = 100 # 임베딩 사이즈 (임베딩시에 변동)\n",
        "hidden_size = 400 # 히든 사이즈 u  (모델에서 변동)\n",
        "attention_dim = 300 #da, (모델에서 변동)\n",
        "num_layers = 1 # 몇층의 LSTM? (현재 참고논문 모델은 1)\n",
        "r = 1 # number of attentions\n",
        "\n",
        "batch_size = 20\n",
        "epochs = 3\n",
        "learning_rate = 0.00005\n",
        "\n",
        "#lstm input : (batch#, seq_len, embedding_size)\n",
        "#lstm 정의할때 : input_size = embedding_size , hidden_size = hidden_size, num_layers = 몇층?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZT7fdapmCXxA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module): \n",
        "  # input_size = embedding_size , hidden_size = sequence_length , num_layers = embedding_dimension\n",
        "    def __init__(self, input_size, hidden_size, num_layers, attention_dim, batch_size, r):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_size = batch_size\n",
        "        self.lstm = nn.LSTM(input_size = input_size ,hidden_size = hidden_size ,\\\n",
        "                            num_layers = num_layers, batch_first=True, bidirectional=True) #\n",
        "        \n",
        "        self.attention1 = nn.Parameter(torch.randn(attention_dim,2*hidden_size)) # da by 2u W1\n",
        "        self.attention2 = nn.Parameter(torch.randn(r,attention_dim)) # r by da W2\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.softmax = nn.Softmax() \n",
        "        \n",
        "        self.linear = nn.Linear(r*2*hidden_size,4)\n",
        "        \n",
        "        \n",
        "        \n",
        "    def init_LSTM(self):\n",
        "        h0 = Variable(torch.zeros(self.num_layers*2, self.batch_size , self.hidden_size)).to(device)\n",
        "        c0 = Variable(torch.zeros(self.num_layers*2, self.batch_size , self.hidden_size)).to(device)\n",
        "        return h0, c0\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h0, c0 = self.init_LSTM()\n",
        "        h, _ = self.lstm(x, (h0, c0))\n",
        "        \n",
        "        att = torch.matmul(self.attention1,torch.transpose(h,1,2)) # torch.mm, -> 2d dot 2d / 3d dot 2d -> matmul / bmm 3d dot 3d\n",
        "        att = self.tanh(att)\n",
        "        att = torch.matmul(self.attention2, att)\n",
        "        att = self.softmax(att) # batchsize by r by n\n",
        "                                # batchsize by n by 2u\n",
        "        out = torch.bmm(att,h)\n",
        "        out = out.reshape(batch_size,-1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7wP7QHBq6ZbH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = SelfAttention(input_size, hidden_size, num_layers,attention_dim, batch_size, r).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) \n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, drop_last= True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fLVFZ8d8rh9Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#iteration"
      ]
    },
    {
      "metadata": {
        "id": "nf7wxb4RjqED",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1603
        },
        "outputId": "c9d28be3-d74d-4010-825f-f4b96ecfb69f"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    batch_loss = 0\n",
        "    i = 0\n",
        "    for step, (X_batch, y_batch) in enumerate(train_loader):\n",
        "        i+=1\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(X_batch)\n",
        "        loss = criterion(y_pred,y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        _loss = loss.data[0]\n",
        "        epoch_loss += _loss\n",
        "        batch_loss += _loss\n",
        "        if i % 1000 == 999 : \n",
        "            print('num_iter =',i+1,'/',len(train_loader), '     loss = ',batch_loss.item()/1000)\n",
        "            batch_loss = 0\n",
        "    print('epoch =',epoch+1,'     epoch_loss =',epoch_loss.item()/len(train_loader) )"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_iter = 1000 / 27507      loss =  1.2830445556640624\n",
            "num_iter = 2000 / 27507      loss =  1.201669921875\n",
            "num_iter = 3000 / 27507      loss =  1.16010986328125\n",
            "num_iter = 4000 / 27507      loss =  1.116328369140625\n",
            "num_iter = 5000 / 27507      loss =  1.080173095703125\n",
            "num_iter = 6000 / 27507      loss =  1.0642777099609375\n",
            "num_iter = 7000 / 27507      loss =  1.0456845703125\n",
            "num_iter = 8000 / 27507      loss =  1.035610107421875\n",
            "num_iter = 9000 / 27507      loss =  1.0186367797851563\n",
            "num_iter = 10000 / 27507      loss =  1.0190184326171876\n",
            "num_iter = 11000 / 27507      loss =  1.0194674682617189\n",
            "num_iter = 12000 / 27507      loss =  1.0029635620117188\n",
            "num_iter = 13000 / 27507      loss =  0.99550634765625\n",
            "num_iter = 14000 / 27507      loss =  1.0018256225585938\n",
            "num_iter = 15000 / 27507      loss =  0.9854664916992187\n",
            "num_iter = 16000 / 27507      loss =  0.9774651489257813\n",
            "num_iter = 17000 / 27507      loss =  0.97765380859375\n",
            "num_iter = 18000 / 27507      loss =  0.9689048461914063\n",
            "num_iter = 19000 / 27507      loss =  0.9681875610351562\n",
            "num_iter = 20000 / 27507      loss =  0.966965087890625\n",
            "num_iter = 21000 / 27507      loss =  0.9557433471679687\n",
            "num_iter = 22000 / 27507      loss =  0.9452518920898437\n",
            "num_iter = 23000 / 27507      loss =  0.9560408935546875\n",
            "num_iter = 24000 / 27507      loss =  0.9411797485351563\n",
            "num_iter = 25000 / 27507      loss =  0.9335\n",
            "num_iter = 26000 / 27507      loss =  0.9462053833007813\n",
            "num_iter = 27000 / 27507      loss =  0.9396322631835937\n",
            "epoch = 1      epoch_loss = 1.0172764972325226\n",
            "num_iter = 1000 / 27507      loss =  0.925046142578125\n",
            "num_iter = 2000 / 27507      loss =  0.9205009765625\n",
            "num_iter = 3000 / 27507      loss =  0.9170584716796875\n",
            "num_iter = 4000 / 27507      loss =  0.9260318603515625\n",
            "num_iter = 5000 / 27507      loss =  0.9137068481445313\n",
            "num_iter = 6000 / 27507      loss =  0.914815185546875\n",
            "num_iter = 7000 / 27507      loss =  0.9065234985351562\n",
            "num_iter = 8000 / 27507      loss =  0.9116678466796875\n",
            "num_iter = 9000 / 27507      loss =  0.9062171020507812\n",
            "num_iter = 10000 / 27507      loss =  0.9070081176757813\n",
            "num_iter = 11000 / 27507      loss =  0.9031983642578125\n",
            "num_iter = 12000 / 27507      loss =  0.9018013916015625\n",
            "num_iter = 13000 / 27507      loss =  0.8939034423828125\n",
            "num_iter = 14000 / 27507      loss =  0.9037109985351562\n",
            "num_iter = 15000 / 27507      loss =  0.892804931640625\n",
            "num_iter = 16000 / 27507      loss =  0.897428466796875\n",
            "num_iter = 17000 / 27507      loss =  0.8899922485351562\n",
            "num_iter = 18000 / 27507      loss =  0.8888489379882812\n",
            "num_iter = 19000 / 27507      loss =  0.8902720336914063\n",
            "num_iter = 20000 / 27507      loss =  0.8828328247070313\n",
            "num_iter = 21000 / 27507      loss =  0.8906170654296875\n",
            "num_iter = 22000 / 27507      loss =  0.8859959106445312\n",
            "num_iter = 23000 / 27507      loss =  0.8780615234375\n",
            "num_iter = 24000 / 27507      loss =  0.8762498779296874\n",
            "num_iter = 25000 / 27507      loss =  0.87657861328125\n",
            "num_iter = 26000 / 27507      loss =  0.8796893920898438\n",
            "num_iter = 27000 / 27507      loss =  0.8688228149414062\n",
            "epoch = 2      epoch_loss = 0.8977750265841422\n",
            "num_iter = 1000 / 27507      loss =  0.8715767211914063\n",
            "num_iter = 2000 / 27507      loss =  0.8671990356445313\n",
            "num_iter = 3000 / 27507      loss =  0.8634315185546875\n",
            "num_iter = 4000 / 27507      loss =  0.8663609619140625\n",
            "num_iter = 5000 / 27507      loss =  0.8666239013671875\n",
            "num_iter = 6000 / 27507      loss =  0.8712598876953125\n",
            "num_iter = 7000 / 27507      loss =  0.868970703125\n",
            "num_iter = 8000 / 27507      loss =  0.8625726318359375\n",
            "num_iter = 9000 / 27507      loss =  0.8597366333007812\n",
            "num_iter = 10000 / 27507      loss =  0.867734130859375\n",
            "num_iter = 11000 / 27507      loss =  0.86137646484375\n",
            "num_iter = 12000 / 27507      loss =  0.8641701049804688\n",
            "num_iter = 13000 / 27507      loss =  0.86469384765625\n",
            "num_iter = 14000 / 27507      loss =  0.8576651000976563\n",
            "num_iter = 15000 / 27507      loss =  0.8583003540039063\n",
            "num_iter = 16000 / 27507      loss =  0.85582177734375\n",
            "num_iter = 17000 / 27507      loss =  0.853685791015625\n",
            "num_iter = 18000 / 27507      loss =  0.8565540771484375\n",
            "num_iter = 19000 / 27507      loss =  0.85353955078125\n",
            "num_iter = 20000 / 27507      loss =  0.8512630615234374\n",
            "num_iter = 21000 / 27507      loss =  0.8586500854492187\n",
            "num_iter = 22000 / 27507      loss =  0.8501294555664063\n",
            "num_iter = 23000 / 27507      loss =  0.8491116333007812\n",
            "num_iter = 24000 / 27507      loss =  0.8533910522460938\n",
            "num_iter = 25000 / 27507      loss =  0.847619384765625\n",
            "num_iter = 26000 / 27507      loss =  0.8520250854492187\n",
            "num_iter = 27000 / 27507      loss =  0.8460192260742188\n",
            "epoch = 3      epoch_loss = 0.8591112887174174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y2SSXmgx38-1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#성능 \n",
        "\n",
        "56.63 epochs = 20  lr = 5e-05  batch size = 20  hidden size = 200 da = 30\n"
      ]
    },
    {
      "metadata": {
        "id": "VEM_yTJVrpxK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "1898e02e-b5d7-40d8-9415-e27a8b7279ee"
      },
      "cell_type": "code",
      "source": [
        "### 10. test set accuracy\n",
        "model.eval() #평가 모드\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for (inputs, labels) in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    test_acc = 100*correct/total\n",
        "print(test_acc,'epochs =',epochs, ' lr =',learning_rate,' batch size =',batch_size, ' hidden size =',hidden_size, 'da =',attention_dim)\n",
        "model.train()# 평가 모드 원상복구"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "61.43 epochs = 3  lr = 5e-05  batch size = 20  hidden size = 400 da = 300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SelfAttention(\n",
              "  (lstm): LSTM(100, 400, batch_first=True, bidirectional=True)\n",
              "  (tanh): Tanh()\n",
              "  (softmax): Softmax()\n",
              "  (linear): Linear(in_features=800, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "C5GXMryf7ELR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}